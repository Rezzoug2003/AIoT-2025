{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrlARzyP1yo3",
        "outputId": "5843de12-2d90-40bc-d247-0e7eaac76a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/kaggle/input/uci-air-quality-dataset/AirQualityUCI.csv\", sep=None, engine='python')\n",
        "\n",
        "df = df.dropna(axis=1, how='all')\n",
        "df.columns = df.columns.str.strip().str.replace('\"', '').str.replace('\\ufeff', '')\n",
        "\n",
        "print(\"✅ Auto-detected columns:\")\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wF1s3Aj9Ctz",
        "outputId": "b357c221-f38f-4504-b505-4dd70e8ff94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Auto-detected columns:\n",
            "['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Benzene concentration as target (C6H6(GT))\n",
        "target_col = 'C6H6(GT)'\n",
        "\n",
        "# Drop rows with missing target values\n",
        "df = df.dropna(subset=[])\n",
        "\n",
        "# Binary classification: high or low pollution\n",
        "threshold = df[target_col].median()\n",
        "df['AirQuality'] = (df[target_col] > threshold).astype(int)\n",
        "\n",
        "# Prepare X and y\n",
        "X = df.drop(columns=['Date', 'Time', 'AirQuality'], errors='ignore')\n",
        "y = df['AirQuality']\n",
        "\n",
        "# Drop any remaining NaNs\n",
        "X = X.dropna()\n",
        "y = y.loc[X.index]\n",
        "\n",
        "print(\"✅ X shape:\", X.shape, \" | y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gua4rIBl6kuT",
        "outputId": "f7a3a950-c049-4149-bb8d-d11d2496dd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ X shape: (9357, 13)  | y shape: (9357,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ab1ddf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHQ75Z_s92lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ddf718-9bdf-4a9e-aabf-3fe92bd15d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Logistic Regression Accuracy: 0.989\n",
            "🔹 Logistic Regression F1 Score: 0.989\n",
            "🔹 Training Time: 0.034 seconds\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import time\n",
        "\n",
        "# Define pipeline\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Train\n",
        "start = time.time()\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "lr_time = time.time() - start\n",
        "\n",
        "# Evaluate\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"🔹 Logistic Regression Accuracy: {lr_acc:.3f}\")\n",
        "print(f\"🔹 Logistic Regression F1 Score: {lr_f1:.3f}\")\n",
        "print(f\"🔹 Training Time: {lr_time:.3f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define pipeline\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # kept for MLOps consistency\n",
        "    ('classifier', XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train\n",
        "start = time.time()\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start\n",
        "\n",
        "# Evaluate\n",
        "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"🌳 XGBoost Accuracy: {xgb_acc:.3f}\")\n",
        "print(f\"🌳 XGBoost F1 Score: {xgb_f1:.3f}\")\n",
        "print(f\"🌳 Training Time: {xgb_time:.3f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4DOnyRw0IVz",
        "outputId": "78e13a46-bcc6-4b59-c457-52871df9cf1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌳 XGBoost Accuracy: 1.000\n",
            "🌳 XGBoost F1 Score: 1.000\n",
            "🌳 Training Time: 0.087 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [10:00:30] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Logistic Regression runtime\n",
        "start_time = time.time()\n",
        "_ = lr_pipeline.predict(X_test)\n",
        "lr_time = time.time() - start_time\n",
        "\n",
        "# XGBoost runtime\n",
        "start_time = time.time()\n",
        "_ = xgb_pipeline.predict(X_test)\n",
        "xgb_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nRuntime (seconds):\")\n",
        "print(f\"LR-Pipeline: {lr_time:.5f}\")\n",
        "print(f\"XGB-Pipeline: {xgb_time:.5f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXOEN7J90UZO",
        "outputId": "f36c0770-fac0-45d9-dce0-9c0d16eea6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Runtime (seconds):\n",
            "LR-Pipeline: 0.00340\n",
            "XGB-Pipeline: 0.00651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DsOrCA90sVQ",
        "outputId": "e39759aa-55b3-40ea-f5bd-83daee18aa6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.12/dist-packages (0.61.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory_profiler) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, os, psutil\n",
        "\n",
        "# Save models\n",
        "joblib.dump(lr_pipeline, \"lr_pipeline.joblib\")\n",
        "joblib.dump(xgb_pipeline, \"xgb_pipeline.joblib\")\n",
        "\n",
        "# Model sizes in KB\n",
        "lr_size = os.path.getsize(\"lr_pipeline.joblib\") / 1024\n",
        "xgb_size = os.path.getsize(\"xgb_pipeline.joblib\") / 1024\n",
        "\n",
        "# Current system memory usage\n",
        "mem_usage = psutil.virtual_memory().percent\n",
        "\n",
        "print(\"\\n💾 Resource Summary:\")\n",
        "print(f\"• Logistic Regression model size: {lr_size:.2f} KB\")\n",
        "print(f\"• XGBoost model size: {xgb_size:.2f} KB\")\n",
        "print(f\"• Current system RAM usage: {mem_usage:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0OX1jMG0hS9",
        "outputId": "0e634fcb-d016-47c0-9833-f582395cdac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Resource Summary:\n",
            "• Logistic Regression model size: 2.09 KB\n",
            "• XGBoost model size: 78.67 KB\n",
            "• Current system RAM usage: 10.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# TP4 - Model Preparation Script\n",
        "# Generates: lr_pipeline.pkl, xgb_pipeline.pkl\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "import pickle\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load and clean dataset\n",
        "# ----------------------------\n",
        "df = pd.read_csv(\"/kaggle/input/uci-air-quality-dataset/AirQualityUCI.csv\", sep=None, engine='python')\n",
        "\n",
        "df = df.dropna(axis=1, how='all')\n",
        "df.columns = df.columns.str.strip().str.replace('\"', '').str.replace('\\ufeff', '')\n",
        "\n",
        "print(\"✅ Auto-detected columns:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# df = pd.read_csv(file_path, sep=\";\", skiprows=1, names=columns)\n",
        "df = df.dropna(axis=1, how=\"all\")  # drop empty columns\n",
        "df = df.dropna()  # drop missing rows\n",
        "\n",
        "# Clean column names\n",
        "df.columns = [c.strip().replace('\"', \"\") for c in df.columns]\n",
        "\n",
        "print(\"✅ Cleaned columns:\", df.columns.tolist()[:10], \"...\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Select features + target\n",
        "# ----------------------------\n",
        "# Target = Benzene concentration (C6H6(GT))\n",
        "target_col = \"C6H6(GT)\"\n",
        "if target_col not in df.columns:\n",
        "    raise ValueError(f\"Column '{target_col}' not found! Check dataset structure.\")\n",
        "\n",
        "df = df[df[target_col] != -200]  # remove invalid sensor values\n",
        "\n",
        "# Create binary target: high pollution (1) if above median\n",
        "threshold = df[target_col].median()\n",
        "df[\"target\"] = (df[target_col] > threshold).astype(int)\n",
        "\n",
        "# Select numeric sensor features\n",
        "features = [\n",
        "    \"CO(GT)\", \"PT08.S1(CO)\", \"NMHC(GT)\",\n",
        "    \"PT08.S2(NMHC)\", \"NOx(GT)\", \"PT08.S3(NOx)\",\n",
        "    \"NO2(GT)\", \"PT08.S4(NO2)\", \"PT08.S5(O3)\",\n",
        "    \"T\", \"RH\", \"AH\"\n",
        "]\n",
        "\n",
        "X = df[features]\n",
        "y = df[\"target\"]\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Train/test split\n",
        "# ----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Logistic Regression Pipeline\n",
        "# ----------------------------\n",
        "lr_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "print(\"\\n🔹 Logistic Regression Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_lr))\n",
        "\n",
        "# Save model\n",
        "with open(\"TP4/models/lr_pipeline.pkl\", \"wb\") as f:\n",
        "    pickle.dump(lr_pipeline, f)\n",
        "print(\"✅ Saved: TP4/models/lr_pipeline.pkl\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5. XGBoost Pipeline\n",
        "# ----------------------------\n",
        "xgb_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),  # for consistency in MLOps\n",
        "    (\"model\", XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
        "print(\"\\n🔹 XGBoost Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_xgb))\n",
        "\n",
        "# Save model\n",
        "with open(\"TP4/models/xgb_pipeline.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_pipeline, f)\n",
        "print(\"✅ Saved: TP4/models/xgb_pipeline.pkl\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Optional: Model sizes\n",
        "# ----------------------------\n",
        "import os\n",
        "lr_size = os.path.getsize(\"TP4/models/lr_pipeline.pkl\") / 1024\n",
        "xgb_size = os.path.getsize(\"TP4/models/xgb_pipeline.pkl\") / 1024\n",
        "print(f\"\\n📦 Model sizes -> Logistic: {lr_size:.2f} KB | XGBoost: {xgb_size:.2f} KB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "U61VBiLkQ34C",
        "outputId": "effbb81a-9d22-4cdd-d61f-21a11caf0d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Auto-detected columns:\n",
            "['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
            "✅ Cleaned columns: ['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)'] ...\n",
            "\n",
            "🔹 Logistic Regression Results:\n",
            "Accuracy: 0.9872151195108394\n",
            "F1 Score: 0.9871866295264624\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'TP4/models/lr_pipeline.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3547760389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TP4/models/lr_pipeline.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Saved: TP4/models/lr_pipeline.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TP4/models/lr_pipeline.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 🧠 TP4 - Logistic Regression & XGBoost Pipelines\n",
        "# ============================================================\n",
        "\n",
        "# 1️⃣ Install required libraries\n",
        "!pip install xgboost pandas scikit-learn\n",
        "\n",
        "# 2️⃣ Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 3️⃣ Load and clean dataset\n",
        "# (Assuming your file name is AirQualityUCI.csv from Kaggle)\n",
        "df = pd.read_csv(\"/kaggle/input/uci-air-quality-dataset/AirQualityUCI.csv\", sep=None, engine='python')\n",
        "\n",
        "df = df.dropna(axis=1, how='all')\n",
        "df.columns = df.columns.str.strip().str.replace('\"', '').str.replace('\\ufeff', '')\n",
        "\n",
        "# Clean column names\n",
        "df.columns = df.columns.str.strip().str.replace('\"', '')\n",
        "\n",
        "# Remove empty column names\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "print(\"✅ Cleaned Columns:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Drop missing target values\n",
        "df = df.dropna(subset=['C6H6(GT)'])\n",
        "\n",
        "# Binary classification target\n",
        "df['PollutionLevel'] = (df['C6H6(GT)'] > df['C6H6(GT)'].median()).astype(int)\n",
        "\n",
        "# Feature selection\n",
        "features = ['PT08.S1(CO)', 'PT08.S2(NMHC)', 'PT08.S3(NOx)',\n",
        "            'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
        "X = df[features]\n",
        "y = df['PollutionLevel']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 4️⃣ Logistic Regression Pipeline\n",
        "# ============================================================\n",
        "lr_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "\n",
        "print(\"\\n🔹 Logistic Regression Results:\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_lr), 4))\n",
        "print(\"F1 Score:\", round(f1_score(y_test, y_pred_lr), 4))\n",
        "\n",
        "# ============================================================\n",
        "# 5️⃣ XGBoost Pipeline\n",
        "# ============================================================\n",
        "xgb_pipeline = Pipeline([\n",
        "    (\"model\", XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
        "\n",
        "print(\"\\n🔹 XGBoost Results:\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_xgb), 4))\n",
        "print(\"F1 Score:\", round(f1_score(y_test, y_pred_xgb), 4))\n",
        "\n",
        "# ============================================================\n",
        "# 6️⃣ Save Models\n",
        "# ============================================================\n",
        "os.makedirs(\"TP4/models\", exist_ok=True)\n",
        "\n",
        "with open(\"TP4/models/lr_pipeline.pkl\", \"wb\") as f:\n",
        "    pickle.dump(lr_pipeline, f)\n",
        "\n",
        "with open(\"TP4/models/xgb_pipeline.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_pipeline, f)\n",
        "\n",
        "print(\"\\n✅ Models saved successfully!\")\n",
        "print(\"📂 lr_pipeline.pkl and xgb_pipeline.pkl are in TP4/models/\")\n",
        "\n",
        "# ============================================================\n",
        "# 7️⃣ Resource Analysis (File size, etc.)\n",
        "# ============================================================\n",
        "lr_size = os.path.getsize(\"TP4/models/lr_pipeline.pkl\") / 1024\n",
        "xgb_size = os.path.getsize(\"TP4/models/xgb_pipeline.pkl\") / 1024\n",
        "\n",
        "print(f\"\\n📦 Model Sizes:\")\n",
        "print(f\"   • Logistic Regression: {lr_size:.2f} KB\")\n",
        "print(f\"   • XGBoost: {xgb_size:.2f} KB\")\n",
        "\n",
        "# Optional: Download models from Colab\n",
        "from google.colab import files\n",
        "files.download(\"TP4/models/lr_pipeline.pkl\")\n",
        "files.download(\"TP4/models/xgb_pipeline.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "M7ZdlLYeTdme",
        "outputId": "a5d04881-ae49-4897-95e3-f63c309d2e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "✅ Cleaned Columns:\n",
            "['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
            "\n",
            "🔹 Logistic Regression Results:\n",
            "Accuracy: 0.9936\n",
            "F1 Score: 0.9936\n",
            "\n",
            "🔹 XGBoost Results:\n",
            "Accuracy: 0.9995\n",
            "F1 Score: 0.9995\n",
            "\n",
            "✅ Models saved successfully!\n",
            "📂 lr_pipeline.pkl and xgb_pipeline.pkl are in TP4/models/\n",
            "\n",
            "📦 Model Sizes:\n",
            "   • Logistic Regression: 1.48 KB\n",
            "   • XGBoost: 87.24 KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6e22118c-4607-43b8-abc6-4e4be74b7b83\", \"lr_pipeline.pkl\", 1516)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_34a7f76d-ed98-42a0-8a16-76111c113c2b\", \"xgb_pipeline.pkl\", 89335)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}